<div align="center">
    <h1>Awesome RAG for MVLMs</h1>
    <a href="https://awesome.re"><img src="https://awesome.re/badge.svg"/></a>
</div>

\
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/hee9joon/Awesome-Diffusion-Models) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Made With Love](https://img.shields.io/badge/Made%20With-Love-red.svg)](https://github.com/chetanraj/awesome-github-badges)



This repo aims to record advanced papers on Retrieval Augmented Generation (RAG) in MVLMs.

We strongly encourage the researchers that want to promote their fantastic work to the RAG for MVLMs to make pull request to update their paper's information!

## Contents

- [Resources](#resources)
  - [Workshops and Tutorials](#workshops-and-tutorials)
- [Papers](#papers)
  - [Survey and Benchmark](#survey-and-benchmark)

# Resources 

## Workshops and Tutorials


# Papers 

## Survey and Benchmark 

## Retrieval-enhanced MVLMs

### (Long) Video Understanding

**Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension**  \
Yongdong Luo, Xiawu Zheng, Xiao Yang, Guilin Li, Haojia Lin, Jinfa Huang, Jiayi Ji, Fei Chao, Jiebo Luo, Rongrong Ji \
Arxiv 24 – Nov 2024 [[paper](https://arxiv.org/pdf/2411.13093)]

**ViTA: An Efficient Video-to-Text Algorithm using VLM for RAG-based Video Analysis System**  \
Md Adnan Arefeen, Biplob Debnath, Md Yusuf Sarwar Uddin, Srimat Chakradhar \
CVPRW 24 [[paper](https://aclanthology.org/2024.emnlp-main.62.pdf)]

### Multi-modal Documents

**VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents**  \
Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, Maosong Sun \
Arxiv 24 – Oct 2024 [[paper](https://arxiv.org/abs/2410.10594)]

### Medical Vision

**Mmed-rag: Versatile multimodal rag system for medical vision language models**  \
Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, Huaxiu Yao \
Arxiv 24 – Oct 2024 [[paper](https://arxiv.org/pdf/2410.13085)]

**Rule: Reliable multimodal rag for factuality in medical vision language models**  \
Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, Huaxiu Yao \
EMNLP 24 [[paper](https://aclanthology.org/2024.emnlp-main.62.pdf)]

### Visual Spacial Understanding

**RAG-Guided Large Language Models for Visual Spatial Description with Adaptive Hallucination Corrector**  \
Jun Yu, Yunxiang Zhang, Zerui Zhang, Zhao Yang, Gongpeng Zhao, Fengzhao Sun, Fanrui Zhang, Qingsong Liu, Jianqing Sun, Jiaen Liang, Yaohui Zhang \
ACM MM 24 [[paper](https://dl.acm.org/doi/abs/10.1145/3664647.3688990?casa_token=SlLR5jgRRkgAAAAA:DzC124tFMWQSMYkKRGkPTwU-aaT7TSv_iVjE-dsZtbna9j3zCYX1A6qcfgmpEKTms8DoZDgplc5u8g)]

### Driving Scenarios

**RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model**  \
Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, Matthew Gadd \
Arxiv 24 - May 2024 [[paper](https://arxiv.org/abs/2402.10828)]
