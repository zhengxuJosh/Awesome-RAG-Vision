<div align="center">
    <h1>Awesome RAG for Vision</h1>
    <a href="https://awesome.re"><img src="https://awesome.re/badge.svg"/></a>
</div>

\
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/hee9joon/Awesome-Diffusion-Models) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
[![Made With Love](https://img.shields.io/badge/Made%20With-Love-red.svg)](https://github.com/chetanraj/awesome-github-badges)



This repo aims to record advanced papers on Retrieval Augmented Generation (RAG) in Vision (Computer Vision & Robotics & MVLMs).

We strongly encourage the researchers that want to promote their fantastic work to the RAG for Vision to make pull request to update their paper's information!

## Contents

- [Resources](#resources)
  - [Workshops and Tutorials](#workshops-and-tutorials)
- [Papers](#papers)
  - [Survey and Benchmark](#survey-and-benchmark)

# Resources 

## Workshops and Tutorials


# Papers 

## Survey and Benchmark 

## Retrieval-enhanced MVLMs

### (Long) Video Understanding

**Video-RAG: Visually-aligned Retrieval-Augmented Long Video Comprehension**  \
Yongdong Luo, Xiawu Zheng, Xiao Yang, Guilin Li, Haojia Lin, Jinfa Huang, Jiayi Ji, Fei Chao, Jiebo Luo, Rongrong Ji \
Arxiv 24 – Nov 2024 [[paper](https://arxiv.org/pdf/2411.13093)]

**ViTA: An Efficient Video-to-Text Algorithm using VLM for RAG-based Video Analysis System**  \
Md Adnan Arefeen, Biplob Debnath, Md Yusuf Sarwar Uddin, Srimat Chakradhar \
CVPRW 24 [[paper](https://aclanthology.org/2024.emnlp-main.62.pdf)]

**iRAG: Advancing RAG for Videos with an Incremental Approach**  \
Md Adnan Arefeen, Md Yusuf Sarwar Uddin, Biplob Debnath, Srimat Chakradhar \
CIKM 24 [[paper](https://dl.acm.org/doi/pdf/10.1145/3627673.3680088?casa_token=CDXIXZP0y9QAAAAA:obaFKtQODdGsI3pB22GWuGH2dODwF7N0dj1dl58WfSwavmvrp_1eeaHXj6c2XCQyt-9vF1r1QrUd)]

### Multi-modal Documents

**VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality Documents**  \
Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, Maosong Sun \
Arxiv 24 – Oct 2024 [[paper](https://arxiv.org/abs/2410.10594)]

**Beyond Text: Optimizing RAG with Multimodal Inputs for Industrial Applications**  \
Monica Riedler, Stefan Langer \
Arxiv 24 – Oct 2024 [[paper](https://arxiv.org/pdf/2410.21943)]

### Medical Vision

**Mmed-rag: Versatile multimodal rag system for medical vision language models**  \
Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, Huaxiu Yao \
Arxiv 24 – Oct 2024 [[paper](https://arxiv.org/pdf/2410.13085)]

**Rule: Reliable multimodal rag for factuality in medical vision language models**  \
Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, Huaxiu Yao \
EMNLP 24 [[paper](https://aclanthology.org/2024.emnlp-main.62.pdf)]

### Visual Spacial Understanding

**RAG-Guided Large Language Models for Visual Spatial Description with Adaptive Hallucination Corrector**  \
Jun Yu, Yunxiang Zhang, Zerui Zhang, Zhao Yang, Gongpeng Zhao, Fengzhao Sun, Fanrui Zhang, Qingsong Liu, Jianqing Sun, Jiaen Liang, Yaohui Zhang \
ACM MM 24 [[paper](https://dl.acm.org/doi/abs/10.1145/3664647.3688990?casa_token=SlLR5jgRRkgAAAAA:DzC124tFMWQSMYkKRGkPTwU-aaT7TSv_iVjE-dsZtbna9j3zCYX1A6qcfgmpEKTms8DoZDgplc5u8g)]

### Driving Scenarios / Scene Understanding

**ENWAR: A RAG-empowered Multi-Modal LLM Framework for Wireless Environment Perception**  \
Ahmad M. Nazar, Abdulkadir Celik, Mohamed Y. Selim, Asmaa Abdallah, Daji Qiao, Ahmed M. Eltawil \
Arxiv 24 - Oct 2024 [[paper](https://arxiv.org/pdf/2410.18104)]

**Embodied-RAG: General Non-parametric Embodied Memory for Retrieval and Generation**  \
Quanting Xie, So Yeon Min, Tianyi Zhang, Kedi Xu, Aarav Bajaj, Ruslan Salakhutdinov, Matthew Johnson-Roberson, Yonatan Bisk \
Arxiv 24 - Oct 2024 [[paper](https://arxiv.org/pdf/2409.18313)]

**RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model**  \
Jianhao Yuan, Shuyang Sun, Daniel Omeiza, Bo Zhao, Paul Newman, Lars Kunze, Matthew Gadd \
Arxiv 24 - May 2024 [[paper](https://arxiv.org/abs/2402.10828)]
